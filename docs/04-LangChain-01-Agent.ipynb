{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4155ad",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangChain是构建大模型驱动的代理和应用程序的最简单的方式。\n",
    "\n",
    "## Agent\n",
    "\n",
    "Agent将LLM和Tool结合，能够对任务进行推理、决定哪些工具迭代的完成任务。\n",
    "\n",
    "### 模型\n",
    "\n",
    "Agent推理的核心是模型，LangChain支持静态模型和动态模型。静态模型指的是运行Agent时制定某个LLM；而动态模型则会根据运行时的状态和上下文环境，动态的选择合适的模型。\n",
    "\n",
    "#### 静态模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# 工具函数\n",
    "@tool\n",
    "def multiply(a: str, b: str) -> str:\n",
    "    \"\"\"\n",
    "    将两个字符串转换为浮点数并计算它们的乘积。\n",
    "    \n",
    "    Args:\n",
    "        a (str): 第一个数字的字符串表示\n",
    "        b (str): 第二个数字的字符串表示\n",
    "        \n",
    "    Returns:\n",
    "        str: 格式化的字符串，显示乘法表达式和结果\n",
    "        \n",
    "    Example:\n",
    "        >>> multiply(\"2.5\", \"3.2\")\n",
    "        \"2.5 * 3.2 = 8.0\"\n",
    "        \n",
    "    Note:\n",
    "        如果输入字符串无法转换为浮点数，将抛出ValueError异常\n",
    "    \"\"\"\n",
    "    return f\"{float(a)} * {float(b)} = {float(a) * float(b)}\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\", \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[multiply]\n",
    ")\n",
    "\n",
    "print(\"开始流式输出：\\n\" + \"=\"*50)\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"计算3.5乘以2.8\"}]}\n",
    "):\n",
    "    # 提取并格式化输出\n",
    "    for key, value in chunk.items():\n",
    "        if key == 'model':\n",
    "            # 提取AI消息内容\n",
    "            messages = value.get('messages', [])\n",
    "            for msg in messages:\n",
    "                if hasattr(msg, 'content'):\n",
    "                    print(f\"AI: {msg.content}\")\n",
    "        elif key == 'tools':\n",
    "            # 提取工具消息内容\n",
    "            messages = value.get('messages', [])\n",
    "            for msg in messages:\n",
    "                if hasattr(msg, 'content'):\n",
    "                    print(f\"工具调用: {msg.content}\")\n",
    "print(\"=\"*50 + \"\\n流式输出完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff37fa8",
   "metadata": {},
   "source": [
    "#### 动态模型\n",
    "\n",
    "动态模型在运行时根据当前状态和上下文选择。这支持复杂的路由逻辑和成本优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "basic_model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\", \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "advanced_model = ChatOpenAI(\n",
    "    model=\"deepseek-reasoner\", \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 10:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    return handler(request)\n",
    "\n",
    "\n",
    "# 工具函数\n",
    "@tool\n",
    "def multiply(a: str, b: str) -> str:\n",
    "    \"\"\"\n",
    "    将两个字符串转换为浮点数并计算它们的乘积。\n",
    "    \n",
    "    Args:\n",
    "        a (str): 第一个数字的字符串表示\n",
    "        b (str): 第二个数字的字符串表示\n",
    "        \n",
    "    Returns:\n",
    "        str: 格式化的字符串，显示乘法表达式和结果\n",
    "        \n",
    "    Example:\n",
    "        >>> multiply(\"2.5\", \"3.2\")\n",
    "        \"2.5 * 3.2 = 8.0\"\n",
    "        \n",
    "    Note:\n",
    "        如果输入字符串无法转换为浮点数，将抛出ValueError异常\n",
    "    \"\"\"\n",
    "    return f\"{float(a)} * {float(b)} = {float(a) * float(b)}\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\", \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    ")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=llm,\n",
    "    tools=[multiply],\n",
    "    middleware=[dynamic_model_selection]\n",
    ")\n",
    "\n",
    "print(\"开始流式输出：\\n\" + \"=\"*50)\n",
    "\n",
    "for chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"计算3.5乘以2.8\"}]}\n",
    "):\n",
    "    # 提取并格式化输出\n",
    "    for key, value in chunk.items():\n",
    "        if key == 'model':\n",
    "            # 提取AI消息内容\n",
    "            messages = value.get('messages', [])\n",
    "            for msg in messages:\n",
    "                if hasattr(msg, 'content'):\n",
    "                    print(f\"AI: {msg.content}\")\n",
    "        elif key == 'tools':\n",
    "            # 提取工具消息内容\n",
    "            messages = value.get('messages', [])\n",
    "            for msg in messages:\n",
    "                if hasattr(msg, 'content'):\n",
    "                    print(f\"工具调用: {msg.content}\")\n",
    "print(\"=\"*50 + \"\\n流式输出完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1edd5a0",
   "metadata": {},
   "source": [
    "## Tool\n",
    "\n",
    "工具赋予智能体执行操作的能力\n",
    "\n",
    "### 定义工具并传给Agent使用\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    return f\"Weather in {location}: Sunny, 72°F\"\n",
    "\n",
    "agent = create_agent(model, tools=[search, get_weather])\n",
    "```\n",
    "\n",
    "### 工具的错误处理\n",
    "\n",
    "要自定义工具错误的处理方式，请使用 @wrap_tool_call 装饰器创建中间件\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"Handle tool execution errors with custom messages.\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # Return a custom error message to the model\n",
    "        return ToolMessage(\n",
    "            content=f\"Tool error: Please check your input and try again. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors]\n",
    ")\n",
    "```\n",
    "\n",
    "当工具失败时，智能体将返回一个包含自定义错误消息的 ToolMessage\n",
    "```\n",
    "[\n",
    "    ...\n",
    "    ToolMessage(\n",
    "        content=\"Tool error: Please check your input and try again. (division by zero)\",\n",
    "        tool_call_id=\"...\"\n",
    "    ),\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "## 结构化输出\n",
    "\n",
    "在某些情况下，您可能希望智能体以特定格式返回输出。LangChain 通过 response_format 参数提供结构化输出策略。\n",
    "\n",
    "### 工具策略\n",
    "\n",
    "ToolStrategy 使用人工工具调用来生成结构化输出。这适用于任何支持工具调用的模型。\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[search_tool],\n",
    "    response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}]\n",
    "})\n",
    "\n",
    "result[\"structured_response\"]\n",
    "# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')\n",
    "```\n",
    "\n",
    "### 提供者策略\n",
    "\n",
    "ProviderStrategy 使用模型提供者原生的结构化输出生成。这更可靠，但仅适用于支持原生结构化输出的提供者（例如 OpenAI）。\n",
    "\n",
    "```python\n",
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o\",\n",
    "    response_format=ProviderStrategy(ContactInfo)\n",
    ")\n",
    "```\n",
    "\n",
    "## 中间件\n",
    "\n",
    "中间件为在执行的不同阶段自定义智能体行为提供了强大的可扩展性。您可以使用中间件来\n",
    "\n",
    "1. 在模型调用前处理状态（例如，消息截断、上下文注入）\n",
    "2. 修改或验证模型的响应（例如，防护措施、内容过滤）\n",
    "3. 使用自定义逻辑处理工具执行错误\n",
    "4. 根据状态或上下文实现动态模型选择\n",
    "5. 添加自定义日志、监控或分析\n",
    "\n",
    "中间件无缝集成到智能体的执行图中，允许您在关键点拦截和修改数据流，而无需更改核心智能体逻辑。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
